name: Scrape Grants

on:
  schedule:
    # Run daily at 9 AM UTC (4 AM EST / 5 AM EDT)
    - cron: '0 9 * * *'
  workflow_dispatch:
    inputs:
      site:
        description: 'Site to scrape (leave empty for all)'
        required: false
        default: ''
      max_items:
        description: 'Maximum items to scrape (leave empty for all)'
        required: false
        default: ''

permissions:
  contents: write
  issues: write

env:
  PYTHON_VERSION: '3.11'

jobs:
  scrape:
    runs-on: ubuntu-latest
    outputs:
      cookies_valid: ${{ steps.check_cookies.outputs.valid }}
      scrape_success: ${{ steps.scrape.outputs.success }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Set up Chrome for Selenium
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Restore cookies from secret
        id: restore_cookies
        env:
          COOKIES_B64: ${{ secrets.INFOREADY_COOKIES }}
        run: |
          if [ -z "$COOKIES_B64" ]; then
            echo "::error::INFOREADY_COOKIES secret is not set"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi

          # Debug: Show base64 string info (not the actual content for security)
          echo "Base64 string length: ${#COOKIES_B64} characters"
          echo "First 20 chars: ${COOKIES_B64:0:20}..."
          echo "Last 20 chars: ...${COOKIES_B64: -20}"

          # Check for common issues
          if [[ "$COOKIES_B64" == "null" ]] || [[ "$COOKIES_B64" == "undefined" ]]; then
            echo "::error::Secret contains '$COOKIES_B64' instead of base64 data"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi

          # Decode base64 cookies and save to file
          # Use printf to avoid newline issues, and tr to remove any whitespace
          printf '%s' "$COOKIES_B64" | tr -d '[:space:]' | base64 -d > data/cookies.pkl 2>&1 || {
            echo "::error::Base64 decoding failed"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          }

          if [ -f data/cookies.pkl ] && [ -s data/cookies.pkl ]; then
            FILE_SIZE=$(wc -c < data/cookies.pkl)
            echo "Cookies restored successfully ($FILE_SIZE bytes)"
            # Debug: Show first few bytes as hex to verify it's pickle data
            echo "First 20 bytes (hex): $(head -c 20 data/cookies.pkl | xxd -p)"
            echo "valid=true" >> $GITHUB_OUTPUT
          else
            echo "::error::Failed to restore cookies or file is empty"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Check cookie validity
        id: check_cookies
        run: |
          # Test if cookies can be loaded and check for login redirects
          # Note: 404 is OK - the scraper handles it by falling back to Selenium
          python << 'PYTHON_SCRIPT'
          import os
          import pickle
          import requests
          import sys
          from pathlib import Path

          def write_output(key, value):
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f'{key}={value}\n')

          cookies_path = Path('data/cookies.pkl')
          if not cookies_path.exists():
              print('::error::Cookies file not found')
              write_output('valid', 'false')
              sys.exit(1)

          try:
              with open(cookies_path, 'rb') as f:
                  cookies_list = pickle.load(f)
              print(f'Loaded {len(cookies_list)} cookies from file')
          except Exception as e:
              print(f'::error::Failed to load cookies: {e}')
              write_output('valid', 'false')
              sys.exit(1)

          # Create a session with cookies
          session = requests.Session()
          for cookie in cookies_list:
              session.cookies.set(
                  cookie['name'],
                  cookie['value'],
                  domain=cookie.get('domain', ''),
                  path=cookie.get('path', '/')
              )

          # Test request - we mainly check for login redirects
          # 404 is acceptable since scraper falls back to Selenium
          try:
              resp = session.get(
                  'https://umich.infoready4.com/Search/GetFundingOpportunities',
                  params={'pageNumber': 1, 'pageSize': 1},
                  headers={'Accept': 'application/json'},
                  timeout=30,
                  allow_redirects=True
              )

              print(f'Response status: {resp.status_code}')
              print(f'Response URL: {resp.url}')

              # Check for redirect to login page - this means cookies are definitely expired
              if 'weblogin' in resp.url.lower() or 'shibboleth' in resp.url.lower():
                  print('::error::Cookies expired - redirected to login page')
                  write_output('valid', 'false')
                  sys.exit(1)

              if resp.status_code == 200:
                  try:
                      data = resp.json()
                      if 'records' in data or 'Records' in data or isinstance(data, list):
                          print('Cookies are valid - JSON API working')
                          write_output('valid', 'true')
                          sys.exit(0)
                  except:
                      pass

              # 403/404 means JSON API needs Selenium, but cookies might still be valid
              # Let the scraper try with Selenium fallback
              if resp.status_code in (403, 404):
                  print(f'JSON API returned {resp.status_code} - will use Selenium fallback')
                  print('Cookies loaded successfully, proceeding with scraper')
                  write_output('valid', 'true')
                  sys.exit(0)

              # Other errors - still try, let scraper determine
              print(f'::warning::Unexpected response {resp.status_code}, will attempt scrape anyway')
              write_output('valid', 'true')
              sys.exit(0)

          except Exception as e:
              print(f'::warning::Cookie check request failed: {e}')
              print('Will attempt scrape anyway - Selenium may work')
              write_output('valid', 'true')
              sys.exit(0)
          PYTHON_SCRIPT
        continue-on-error: true

      - name: Run scraper
        id: scrape
        if: steps.check_cookies.outputs.valid == 'true'
        run: |
          SITE_ARG=""
          MAX_ITEMS_ARG=""

          if [ -n "${{ github.event.inputs.site }}" ]; then
            SITE_ARG="--site ${{ github.event.inputs.site }}"
          fi

          if [ -n "${{ github.event.inputs.max_items }}" ]; then
            MAX_ITEMS_ARG="--max-items ${{ github.event.inputs.max_items }}"
          fi

          # Run incremental scrape for each enabled site
          echo "Running incremental scrape..."
          python utils/scrape_grants.py --incremental $SITE_ARG $MAX_ITEMS_ARG 2>&1 | tee scrape_output.log

          SCRAPE_EXIT_CODE=${PIPESTATUS[0]}

          if [ $SCRAPE_EXIT_CODE -eq 0 ]; then
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "success=false" >> $GITHUB_OUTPUT
            # Check if failure was due to auth
            if grep -qi "unauthorized\|forbidden\|login\|authentication" scrape_output.log; then
              echo "cookies_expired=true" >> $GITHUB_OUTPUT
            fi
          fi

          exit $SCRAPE_EXIT_CODE
        continue-on-error: true

      - name: Generate statistics
        if: steps.scrape.outputs.success == 'true'
        run: |
          python core/stats.py
          python core/stats.py --json > docs/grants-data.json
          cp docs/grants-data.json website/public/grants-data.json

      - name: Commit and push changes
        if: steps.scrape.outputs.success == 'true'
        run: |
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'

          # Add all database and stats files
          git add output/db/*.json output/db/*.jsonl output/db/*.csv 2>/dev/null || true
          git add docs/grants-data.json website/public/grants-data.json 2>/dev/null || true

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            TIMESTAMP=$(date +"%Y-%m-%d %H:%M")
            git commit -m "Update grant data - $TIMESTAMP"
            git push
          fi

  notify-cookie-expiry:
    needs: scrape
    if: always() && (needs.scrape.outputs.cookies_valid == 'false' || failure())
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Check for existing cookie issue
        id: check_issue
        uses: actions/github-script@v7
        with:
          script: |
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'cookie-refresh-needed'
            });

            const existingIssue = issues.data.find(issue =>
              issue.title.includes('Cookie Refresh Required')
            );

            if (existingIssue) {
              console.log(`Found existing issue #${existingIssue.number}`);
              core.setOutput('exists', 'true');
              core.setOutput('issue_number', existingIssue.number);
            } else {
              core.setOutput('exists', 'false');
            }

      - name: Create cookie refresh issue
        if: steps.check_issue.outputs.exists == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'ðŸ” Cookie Refresh Required - OpTrack Scraper',
              body: `## Cookie Refresh Needed

            The OpTrack scraper failed because the authentication cookies have expired or are invalid.

            ### What happened
            - **Workflow run:** [${context.runId}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
            - **Time:** ${new Date().toISOString()}
            - **Reason:** Cookies expired or authentication failed

            ### How to fix

            1. **On your local machine**, run:
               \`\`\`bash
               cd /path/to/optrack
               source venv/bin/activate
               python core/login_and_save_cookies.py
               \`\`\`

            2. **Complete Duo authentication** in the browser window that opens

            3. **Encode the new cookies** for GitHub:
               \`\`\`bash
               python scripts/encode_cookies.py
               \`\`\`

            4. **Update the GitHub secret:**
               - Go to: [Repository Settings > Secrets](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/settings/secrets/actions)
               - Update \`INFOREADY_COOKIES\` with the base64 string from step 3

            5. **Close this issue** once the secret is updated

            6. **Optionally re-run** the [scraper workflow](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/workflows/scrape-grants.yml) to verify

            ---
            *This issue was automatically created by the OpTrack scraper workflow.*`,
              labels: ['cookie-refresh-needed', 'automated']
            });

            console.log(`Created issue #${issue.data.number}`);

      - name: Comment on existing issue
        if: steps.check_issue.outputs.exists == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: ${{ steps.check_issue.outputs.issue_number }},
              body: `### Scraper still failing

            - **Workflow run:** [${context.runId}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
            - **Time:** ${new Date().toISOString()}

            The cookies still need to be refreshed. Please follow the instructions above.`
            });
